{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b01a2a7",
   "metadata": {},
   "source": [
    "#ouputs summary\n",
    "\n",
    "****** Threshold for considering word as unknown is 2 ******\n",
    "\n",
    "****** Vocabulary size  23183 ******\n",
    "\n",
    "****** < unk > occurence count  20011 ********\n",
    "\n",
    "***** Size of emission dictionary  30303 ******\n",
    "\n",
    "****** Size of transition dictionary  1392 ******\n",
    "\n",
    "GREEDY sentence wise accuracy 93.20391642320469\n",
    "GREEDY word wise accuracy 93.51132293121243\n",
    "\n",
    "VITERBI sentence wise accuracy 94.42883547709638\n",
    "VITERBI word wise accuracy  94.76883613623946"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f413b2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fbe275a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = defaultdict(int)                          # gives default value 0 to new elements\n",
    "pos_freq = defaultdict(int)\n",
    "first_tags = set()\n",
    "train_data = []\n",
    "pos_tags = []\n",
    "sentence = []\n",
    "\n",
    "sentences_count = 0\n",
    "with open('data/train', newline='') as csvfile:    # read train file line by line\n",
    "    spamreader = csv.reader(csvfile, delimiter='\\t')\n",
    "    for row in spamreader:\n",
    "        try:\n",
    "            if len(sentence)==0:\n",
    "                first_tags.add(row[2])\n",
    "            vocab[row[1].strip()]+=1\n",
    "            pos_freq[row[2].strip()]+=1\n",
    "            sentence.append(row[1].strip())           # append words to a get a sentence \n",
    "            pos_tags.append(row[2].strip())           # append pos_tags to get corresponding pos tags\n",
    "            \n",
    "        except:\n",
    "            if len(row)>0:                            # catch if a proper sentence had an error while reading \n",
    "                print(row)\n",
    "            sentences_count+=1                        # count number of sentences \n",
    "            train_data.append([sentences_count, sentence, pos_tags])\n",
    "            sentence = []                             # reset everything for taking in next new sentence\n",
    "            pos_tags = []  \n",
    "            # count sentences \n",
    "            #use pos tags not words row[2]\n",
    "    train_data.append([sentences_count, sentence,pos_tags]) \n",
    "train_df = pd.DataFrame(train_data,columns = ['sentence_number', 'words', 'pos_tags'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38e2f85d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>words</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Pierre, Vinken, ,, 61, years, old, ,, will, j...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[Mr., Vinken, is, chairman, of, Elsevier, N.V....</td>\n",
       "      <td>[NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[Rudolph, Agnew, ,, 55, years, old, and, forme...</td>\n",
       "      <td>[NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[A, form, of, asbestos, once, used, to, make, ...</td>\n",
       "      <td>[DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[The, asbestos, fiber, ,, crocidolite, ,, is, ...</td>\n",
       "      <td>[DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38213</th>\n",
       "      <td>38214</td>\n",
       "      <td>[After, San, Francisco, Mayor, Art, Agnos, spo...</td>\n",
       "      <td>[IN, NNP, NNP, NNP, NNP, NNP, VBD, IN, NN, IN,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38214</th>\n",
       "      <td>38215</td>\n",
       "      <td>[And, the, county, of, Los, Angeles, placed, i...</td>\n",
       "      <td>[CC, DT, NN, IN, NNP, NNP, VBD, PRP$, NNS, CC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38215</th>\n",
       "      <td>38216</td>\n",
       "      <td>[Two, Los, Angeles, radio, stations, initiated...</td>\n",
       "      <td>[CD, NNP, NNP, NN, NNS, VBD, NNP, NNP, NN, NNS...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38216</th>\n",
       "      <td>38217</td>\n",
       "      <td>[The, Los, Angeles, Red, Cross, sent, 2,480, c...</td>\n",
       "      <td>[DT, NNP, NNP, NNP, NNP, VBD, CD, NNS, ,, CD, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38217</th>\n",
       "      <td>38217</td>\n",
       "      <td>[It, is, also, pulling, 20, people, out, of, P...</td>\n",
       "      <td>[PRP, VBZ, RB, VBG, CD, NNS, IN, IN, NNP, NNP,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>38218 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       sentence_number                                              words  \\\n",
       "0                    1  [Pierre, Vinken, ,, 61, years, old, ,, will, j...   \n",
       "1                    2  [Mr., Vinken, is, chairman, of, Elsevier, N.V....   \n",
       "2                    3  [Rudolph, Agnew, ,, 55, years, old, and, forme...   \n",
       "3                    4  [A, form, of, asbestos, once, used, to, make, ...   \n",
       "4                    5  [The, asbestos, fiber, ,, crocidolite, ,, is, ...   \n",
       "...                ...                                                ...   \n",
       "38213            38214  [After, San, Francisco, Mayor, Art, Agnos, spo...   \n",
       "38214            38215  [And, the, county, of, Los, Angeles, placed, i...   \n",
       "38215            38216  [Two, Los, Angeles, radio, stations, initiated...   \n",
       "38216            38217  [The, Los, Angeles, Red, Cross, sent, 2,480, c...   \n",
       "38217            38217  [It, is, also, pulling, 20, people, out, of, P...   \n",
       "\n",
       "                                                pos_tags  \n",
       "0      [NNP, NNP, ,, CD, NNS, JJ, ,, MD, VB, DT, NN, ...  \n",
       "1      [NNP, NNP, VBZ, NN, IN, NNP, NNP, ,, DT, NNP, ...  \n",
       "2      [NNP, NNP, ,, CD, NNS, JJ, CC, JJ, NN, IN, NNP...  \n",
       "3      [DT, NN, IN, NN, RB, VBN, TO, VB, NNP, NN, NNS...  \n",
       "4      [DT, NN, NN, ,, NN, ,, VBZ, RB, JJ, IN, PRP, V...  \n",
       "...                                                  ...  \n",
       "38213  [IN, NNP, NNP, NNP, NNP, NNP, VBD, IN, NN, IN,...  \n",
       "38214  [CC, DT, NN, IN, NNP, NNP, VBD, PRP$, NNS, CC,...  \n",
       "38215  [CD, NNP, NNP, NN, NNS, VBD, NNP, NNP, NN, NNS...  \n",
       "38216  [DT, NNP, NNP, NNP, NNP, VBD, CD, NNS, ,, CD, ...  \n",
       "38217  [PRP, VBZ, RB, VBG, CD, NNS, IN, IN, NNP, NNP,...  \n",
       "\n",
       "[38218 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95953d7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "thres = 1\n",
    "output_text = \"vocab.txt\"\n",
    "res = sum(v if v <= thres else 0 for k, v in vocab.items())            # cound the number of words under threshold\n",
    "vocab = {key:val for key, val in vocab.items() if val > thres}         # thresholding the vocabulary\n",
    "vocab = dict(sorted(vocab.items(), key=lambda x: x[1], reverse= True)) # sorting the dictionary \n",
    "vocab = {'< unk >':res, **vocab}                                # appending < unk > as the first elem of dictionary \n",
    "\n",
    "\n",
    "i=0\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for k,v in vocab.items() :\n",
    "        i+=1\n",
    "        f.write(\"%s\\t%s\\t%s\\n\"%(k,i,v))\n",
    "        #fix unknowns tags \n",
    "pos_freq[\"< * >\"]= sentences_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "63f9ad9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "****** Threshold for considering word as unknown is 2 ******\n",
      "\n",
      "****** Vocabulary size  23183 ******\n",
      "\n",
      "****** < unk > occurence count  20011 ********\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"****** Threshold for considering word as unknown is 2 ******\\n\")\n",
    "print(\"****** Vocabulary size \",len(vocab),\"******\\n\")\n",
    "print(\"****** < unk > occurence count \",res, \"********\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e2b97bc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_cnt = defaultdict(float) \n",
    "emission_cnt = defaultdict(float) \n",
    "\n",
    "\n",
    "for idx, (_, words, pos_tags) in train_df.iterrows():\n",
    "    prev = \"< * >\"\n",
    "    for word, pos_tag in zip(words, pos_tags):\n",
    "        try:\n",
    "            transition_cnt[(prev, pos_tag)] += 1      # insert (Prev_POS, POS) to dictionary or add count\n",
    "\n",
    "            if word in vocab:                        \n",
    "                emission_cnt[(pos_tag, word)] += 1      # insert (POS, word) to dictionary or add count\n",
    "\n",
    "            else:                                       # if word has < threshold occurrence\n",
    "                emission_cnt[(pos_tag,'< unk >')]+=1       # insert (POS, <unk>) to dictionary or add count\n",
    "\n",
    "            prev = pos_tag \n",
    "        except:\n",
    "            print(\"error bantu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1e54aa3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('NNP', 'Pierre'), 6.0), (('NNP', 'Vinken'), 2.0), ((',', ','), 46476.0), (('CD', '61'), 25.0), (('NNS', 'years'), 1130.0), (('JJ', 'old'), 213.0), (('MD', 'will'), 2962.0), (('VB', 'join'), 40.0), (('DT', 'the'), 39517.0), (('NN', 'board'), 297.0)]\n",
      "[(('< * >', 'NNP'), 7563.0), (('NNP', 'NNP'), 33139.0), (('NNP', ','), 12131.0), ((',', 'CD'), 987.0), (('CD', 'NNS'), 5502.0), (('NNS', 'JJ'), 995.0), (('JJ', ','), 1717.0), ((',', 'MD'), 490.0), (('MD', 'VB'), 7541.0), (('VB', 'DT'), 5661.0)]\n"
     ]
    }
   ],
   "source": [
    "print(list(emission_cnt.items())[:10])\n",
    "print(list(transition_cnt.items())[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "40dd97c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(('NNP', 'Pierre'), 6.84868961738654e-05), (('NNP', 'Vinken'), 2.2828965391288468e-05), ((',', ','), 0.9999139414802065), (('CD', '61'), 0.0007168253240050465), (('NNS', 'years'), 0.019530237301024905), (('JJ', 'old'), 0.003613599348534202), (('MD', 'will'), 0.3138709335593939), (('VB', 'join'), 0.0015693044058221193), (('DT', 'the'), 0.5016439225642653), (('NN', 'board'), 0.0023287907538381922)]\n",
      "[(('< * >', 'NNP'), 0.1978962241934218), (('NNP', 'NNP'), 0.3782645420509543), (('NNP', ','), 0.13846908958086018), ((',', 'CD'), 0.021234939759036144), (('CD', 'NNS'), 0.15775891730703062), (('NNS', 'JJ'), 0.017196978862406887), (('JJ', ','), 0.029129343105320303), ((',', 'MD'), 0.010542168674698794), (('MD', 'VB'), 0.7990886934407121), (('VB', 'DT'), 0.22209580603397544)]\n"
     ]
    }
   ],
   "source": [
    "transition = defaultdict(float) \n",
    "emission = defaultdict(float) \n",
    "\n",
    "for key,value in transition_cnt.items():\n",
    "    #transition[key] = (value +1) /(pos_freq[key[0]] + len(pos_freq))\n",
    "    transition[key] = (value) /(pos_freq[key[0]])\n",
    "        \n",
    "for key,value in emission_cnt.items(): \n",
    "    #emission[key] = (value +1)/(pos_freq[key[0]] + len(vocab))\n",
    "    emission[key] = (value)/pos_freq[key[0]]\n",
    "        \n",
    "print(list(emission.items())[:10])\n",
    "print(list(transition.items())[:10])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3625a08e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** Size of emission dictionary  30303 ******\n",
      "\n",
      "***** Size of transition dictionary  1392 *****\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"***** Size of emission dictionary \",len(emission),\"******\\n\")\n",
    "print(\"***** Size of transition dictionary \",len(transition),\"*****\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6364540a",
   "metadata": {},
   "outputs": [],
   "source": [
    "transition_str = dict((\",\".join(k), v) for k,v in transition.items())   # converting to string keys for json file\n",
    "emission_str = dict((\",\".join(k), v) for k,v in emission.items())           # converting to string keys for json file\n",
    "hmm = {\"transition\": transition_str, \"emission\": emission_str}\n",
    "with open(\"hmm.json\", \"w\") as outfile:\n",
    "    json.dump(hmm, outfile)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d10ba76d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>words</th>\n",
       "      <th>pos_tags</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[The, Arizona, Corporations, Commission, autho...</td>\n",
       "      <td>[DT, NNP, NNP, NNP, VBD, DT, CD, NN, NN, NN, I...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[The, ruling, follows, a, host, of, problems, ...</td>\n",
       "      <td>[DT, NN, VBZ, DT, NN, IN, NNS, IN, NNP, NNP, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, Arizona, regulatory, ruling, calls, for,...</td>\n",
       "      <td>[DT, NNP, JJ, NN, VBZ, IN, $, CD, CD, IN, JJ, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[The, company, had, sought, increases, totalin...</td>\n",
       "      <td>[DT, NN, VBD, VBN, NNS, VBG, $, CD, CD, ,, CC,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[The, decision, was, announced, after, trading...</td>\n",
       "      <td>[DT, NN, VBD, VBN, IN, NN, VBD, .]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5522</th>\n",
       "      <td>5523</td>\n",
       "      <td>[But, if, the, board, rejects, a, reduced, bid...</td>\n",
       "      <td>[CC, IN, DT, NN, VBZ, DT, VBN, NN, CC, VBZ, TO...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5523</th>\n",
       "      <td>5524</td>\n",
       "      <td>[The, pilots, could, play, hardball, by, notin...</td>\n",
       "      <td>[DT, NNS, MD, VB, NN, IN, VBG, PRP, VBP, JJ, T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5524</th>\n",
       "      <td>5525</td>\n",
       "      <td>[If, they, were, to, insist, on, a, low, bid, ...</td>\n",
       "      <td>[IN, PRP, VBD, TO, VB, IN, DT, JJ, NN, IN, ,, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5525</th>\n",
       "      <td>5526</td>\n",
       "      <td>[Also, ,, because, UAL, Chairman, Stephen, Wol...</td>\n",
       "      <td>[RB, ,, IN, NNP, NNP, NNP, NNP, CC, JJ, NNP, N...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5526</th>\n",
       "      <td>5527</td>\n",
       "      <td>[That, could, cost, him, the, chance, to, infl...</td>\n",
       "      <td>[DT, MD, VB, PRP, DT, NN, TO, VB, DT, NN, CC, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5527 rows Ã— 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_number                                              words  \\\n",
       "0                   1  [The, Arizona, Corporations, Commission, autho...   \n",
       "1                   2  [The, ruling, follows, a, host, of, problems, ...   \n",
       "2                   3  [The, Arizona, regulatory, ruling, calls, for,...   \n",
       "3                   4  [The, company, had, sought, increases, totalin...   \n",
       "4                   5  [The, decision, was, announced, after, trading...   \n",
       "...               ...                                                ...   \n",
       "5522             5523  [But, if, the, board, rejects, a, reduced, bid...   \n",
       "5523             5524  [The, pilots, could, play, hardball, by, notin...   \n",
       "5524             5525  [If, they, were, to, insist, on, a, low, bid, ...   \n",
       "5525             5526  [Also, ,, because, UAL, Chairman, Stephen, Wol...   \n",
       "5526             5527  [That, could, cost, him, the, chance, to, infl...   \n",
       "\n",
       "                                               pos_tags  \n",
       "0     [DT, NNP, NNP, NNP, VBD, DT, CD, NN, NN, NN, I...  \n",
       "1     [DT, NN, VBZ, DT, NN, IN, NNS, IN, NNP, NNP, ,...  \n",
       "2     [DT, NNP, JJ, NN, VBZ, IN, $, CD, CD, IN, JJ, ...  \n",
       "3     [DT, NN, VBD, VBN, NNS, VBG, $, CD, CD, ,, CC,...  \n",
       "4                    [DT, NN, VBD, VBN, IN, NN, VBD, .]  \n",
       "...                                                 ...  \n",
       "5522  [CC, IN, DT, NN, VBZ, DT, VBN, NN, CC, VBZ, TO...  \n",
       "5523  [DT, NNS, MD, VB, NN, IN, VBG, PRP, VBP, JJ, T...  \n",
       "5524  [IN, PRP, VBD, TO, VB, IN, DT, JJ, NN, IN, ,, ...  \n",
       "5525  [RB, ,, IN, NNP, NNP, NNP, NNP, CC, JJ, NNP, N...  \n",
       "5526  [DT, MD, VB, PRP, DT, NN, TO, VB, DT, NN, CC, ...  \n",
       "\n",
       "[5527 rows x 3 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dev_data = []\n",
    "sentence = []\n",
    "pos_tags = []\n",
    "\n",
    "sentence_count = 1\n",
    "with open('data/dev', newline = '') as tsvfile:                            # read dev file line by line \n",
    "    csv_reader = csv.reader(tsvfile, delimiter = '\\t')\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            sentence.append(row[1])\n",
    "            pos_tags.append(row[2])\n",
    "        except:\n",
    "            dev_data.append([sentence_count, sentence, pos_tags])\n",
    "            sentence_count += 1\n",
    "            sentence = []\n",
    "            pos_tags = []\n",
    "    dev_data.append([sentence_count, sentence,pos_tags]) \n",
    "            \n",
    "dev_df = pd.DataFrame(dev_data,columns = ['sentence_number', 'words', 'pos_tags'])\n",
    "dev_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be0a2ff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5151.380460710523 5527\n",
      "GREEDY sentence wise accuracy 93.20391642320469\n",
      "GREEDY word wise accuracy 93.51132293121243\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def greedy_decoding(df):\n",
    "#     correct_matches = 0\n",
    "#     total_words = 0\n",
    "      with open('greedy.out', 'w') as tsvfile:\n",
    "        prev_tag = '< * >'                                 # start position tag as prev tag             \n",
    "        predicted_tags = []\n",
    "        sentence_accuracies = [] \n",
    "        total_accuracy = 0\n",
    "        total_correct = 0\n",
    "        deno = 0\n",
    "        num = 0\n",
    "        small_no = 0.00000001                             # giving a very small probability to non existent proabilities to preserve matched emission / transition  probabilities\n",
    "\n",
    "        for idx, (_, words, pos_tags) in df.iterrows():   # loop through the data \n",
    "            predicted_tags = []                           # keep track of predicted_tags so far\n",
    "            prev_tag = '< * >'                            # start with prev tag as star position tag\n",
    "            for word in words:                            # go word by word in the sentence\n",
    "                max_p = 0                                 # variable to keep track of max probability seen so far\n",
    "                best_tag = random.choice(list(first_tags))       # randomly choose a start POS tag\n",
    "                for pos_tag in pos_freq.keys():            # check probabilties of each plausible tag\n",
    "                    if word not in vocab:\n",
    "                        word = '< unk >'                   # give < unk > frequency value for words not seen \n",
    "                    if (prev_tag, pos_tag) in transition.keys() and (pos_tag, word) in emission.keys(): \n",
    "                        p = (transition[(prev_tag, pos_tag)] +small_no) * (emission[(pos_tag, word)]+small_no)  \n",
    "\n",
    "                    elif (pos_tag, word) in emission.keys() :         # if emission is found and transition is not matched\n",
    "                        #p = (emission[(pos_tag, word)] ) * 1/(pos_freq[pos_tag]+len(pos_freq)) # trying smoothing \n",
    "                        p = ((emission[(pos_tag, word)] +small_no)  * small_no)\n",
    "\n",
    "                    elif (prev_tag, pos_tag) in transition.keys() : # if transition exits and emission match isnt found\n",
    "                        #p = (transition[(prev_tag, pos_tag)])  * 1/(pos_freq[prev_tag]+len(vocab)) # trying  laplace smoothing \n",
    "                        p = ((transition[(prev_tag, pos_tag)]+small_no)  * small_no)     # multiply by small probablity\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        #p = 1/(pos_freq[pos_tag]+len(pos_freq)) *  1/(pos_freq[prev_tag]+len(vocab))\n",
    "                        p = small_no * small_no\n",
    "                        \n",
    "                    if p > max_p:                                    # update best probability found \n",
    "                            max_p = p\n",
    "                            best_tag = pos_tag\n",
    "\n",
    "\n",
    "                #if max_p == 0:\n",
    "                #    best_tag = max(pos, key=pos.get)\n",
    "\n",
    "                predicted_tags.append(best_tag)                      # add the best_found tag to the predicted tags list\n",
    "                prev_tag = best_tag                                  # update prev tag as curr predicted tag \n",
    "\n",
    "            correct_matches = np.sum(np.array(pos_tags) == np.array(predicted_tags)) # calculate number of tags correctly predicted \n",
    "            total_correct += correct_matches                                         \n",
    "            deno +=len(pos_tags)\n",
    "            accuracy = correct_matches/len(pos_tags)\n",
    "            #print(accuracy)\n",
    "            #print(pos_tags,predicted_tags)\n",
    "            total_accuracy += accuracy \n",
    "            num+=1\n",
    "            sentence_accuracies.append(accuracy)\n",
    "            predicted_tags.append(predicted_tags)\n",
    "            idx = 0\n",
    "            for word, pos_tag in zip(words, predicted_tags):\n",
    "                idx += 1\n",
    "                print (\"%d\\t%s\\t%s\" % (idx, word, pos_tag), file=tsvfile)\n",
    "            print (\"\", file=tsvfile)\n",
    "                \n",
    "        print(total_accuracy, num)\n",
    "        print(\"GREEDY sentence wise accuracy\", total_accuracy/num*100)\n",
    "        print(\"GREEDY word wise accuracy\", total_correct/deno*100)\n",
    "        \n",
    "greedy_decoding(dev_df) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "250321cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VITERBI sentence wise accuracy 94.42883547709638\n",
      "VITERBI word wise accuracy  94.76883613623946\n"
     ]
    }
   ],
   "source": [
    "def viterbi_decoding(df):\n",
    "    \n",
    "    prev_tag= '< * >'          # start position tag as prev tag                                      \n",
    "    predicted_tags = []        # save the predicted tags \n",
    "    sentence_accuracies = []   # append the sentence accuracies\n",
    "    total_accuracy = 0         # add up total accuracy \n",
    "    num = 0                    # count number of sentences \n",
    "    total_correct = 0\n",
    "    deno = 0\n",
    "    small_no = 0.00000001\n",
    "    with open('viterbi.out', 'w') as tsvfile:              # open the .out file \n",
    "        for idx, (_, words, pos_tags) in df.iterrows():    # read the df line by line \n",
    "                predicted_tags = []                        # reset for every new sentence\n",
    "                T = len(words)                             \n",
    "                N = len(pos_freq.items())\n",
    "                viterbi = [[0]*(T) for _ in range(N)]\n",
    "                path_tracker = [[0]*(T) for _ in range(N)]\n",
    "                for i in range(0,len(path_tracker)):\n",
    "                    path_tracker[i][0] = -1\n",
    "\n",
    "                for t, word in enumerate(words):           # for each timestamp , word in words\n",
    "                    if word not in vocab:                  # if word is not in vocab give it  < unk > tag\n",
    "                        word = '< unk >'\n",
    "                    for s1, pos_tag in enumerate(pos_freq.keys()):   # for every state, current tag  \n",
    "                        if pos_tag == '< * >':\n",
    "                            pass\n",
    "                        elif t == 0:                          # handle first word separately\n",
    "                            if ('< * >', pos_tag) in transition.keys() and (pos_tag, word) in emission.keys():\n",
    "                                viterbi[s1][0] = (transition[('< * >', pos_tag)]+small_no) * (emission[(pos_tag, word)]+small_no)\n",
    "                                path_tracker[s1][0] = -1      # to track end when backtracking to find the path\n",
    "                        else:\n",
    "                            for s2, prev_tag in enumerate(pos_freq.keys()): # for every possible prev state, prev tag \n",
    "                                if prev_tag == '< * >':                     # ignore start symbol tag\n",
    "                                    pass\n",
    "                                elif (prev_tag, pos_tag) in transition.keys() and (pos_tag, word) in emission.keys():\n",
    "                                    p = viterbi[s2][t-1]* (transition[(prev_tag, pos_tag)]+small_no) * (emission[(pos_tag, word)]+small_no)\n",
    "                                    if p  > viterbi[s1][t]:              # save the best transition probability in the the viterbi table\n",
    "                                        viterbi[s1][t] = p\n",
    "                                        path_tracker[s1][t] = s2         # save best prev pos in this path\n",
    "\n",
    "                best_path_p = 0\n",
    "                best_path_index = 0\n",
    "                for i in range(N):                                       # find the best path start by looking at the last column in viterbi table\n",
    "                    if viterbi[i][T-1] > best_path_p:                    \n",
    "                        best_path_p = viterbi[i][T-1]\n",
    "                        best_path_index = i\n",
    "\n",
    "                best_path = []\n",
    "                t = T-1\n",
    "                \n",
    "                pos_list = list(pos_freq.keys()) \n",
    "                while best_path_index != -1 and t!=-1:                             # backtrack to find the path that gave the best probability \n",
    "                    best_path.append(pos_list[best_path_index])\n",
    "                    #print(\"index\",best_path_index)\n",
    "                    best_path_index = path_tracker[best_path_index][t]\n",
    "                    t -= 1 \n",
    "                if len(best_path) == 0:\n",
    "                    print(\"best path not found\")\n",
    "\n",
    "\n",
    "\n",
    "                best_path = best_path[::-1]                              # reverse the best path tags found by backtracking                          \n",
    "                correct_matches = np.sum(np.array(pos_tags) == np.array(best_path)) # count correct matches for accuracy calculations\n",
    "                total_correct +=  correct_matches\n",
    "                deno+=len(pos_tags)\n",
    "                accuracy = correct_matches/len(pos_tags)\n",
    "                sentence_accuracies.append(accuracy)\n",
    "                predicted_tags.append(best_path)\n",
    "                total_accuracy += accuracy \n",
    "                num+=1\n",
    "                idx = 0\n",
    "                for word, pos_tag in zip(words, best_path):\n",
    "                    idx += 1\n",
    "                    print (\"%d\\t%s\\t%s\" % (idx, word, pos_tag), file=tsvfile)\n",
    "                print (\"\", file=tsvfile)\n",
    "        \n",
    "    print(\"VITERBI sentence wise accuracy\", total_accuracy/num*100)\n",
    "    print(\"VITERBI word wise accuracy \", total_correct/deno*100)\n",
    "\n",
    "\n",
    "viterbi_decoding(dev_df)   \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76b8505",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentence_number</th>\n",
       "      <th>words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>[Influential, members, of, the, House, Ways, a...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>[The, bill, ,, whose, backers, include, Chairm...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>[The, bill, intends, to, restrict, the, RTC, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>[``, Such, agency, `, self-help, ', borrowing,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>[The, complex, financing, plan, in, the, S&amp;L, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5457</th>\n",
       "      <td>5458</td>\n",
       "      <td>[Says, Peter, Mokaba, ,, president, of, the, S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5458</th>\n",
       "      <td>5459</td>\n",
       "      <td>[They, never, considered, themselves, to, be, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5459</th>\n",
       "      <td>5460</td>\n",
       "      <td>[At, last, night, 's, rally, ,, they, called, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5460</th>\n",
       "      <td>5461</td>\n",
       "      <td>[``, We, emphasize, discipline, because, we, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5461</th>\n",
       "      <td>5462</td>\n",
       "      <td>[``, We, want, to, see, Nelson, Mandela, and, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5462 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      sentence_number                                              words\n",
       "0                   1  [Influential, members, of, the, House, Ways, a...\n",
       "1                   2  [The, bill, ,, whose, backers, include, Chairm...\n",
       "2                   3  [The, bill, intends, to, restrict, the, RTC, t...\n",
       "3                   4  [``, Such, agency, `, self-help, ', borrowing,...\n",
       "4                   5  [The, complex, financing, plan, in, the, S&L, ...\n",
       "...               ...                                                ...\n",
       "5457             5458  [Says, Peter, Mokaba, ,, president, of, the, S...\n",
       "5458             5459  [They, never, considered, themselves, to, be, ...\n",
       "5459             5460  [At, last, night, 's, rally, ,, they, called, ...\n",
       "5460             5461  [``, We, emphasize, discipline, because, we, k...\n",
       "5461             5462  [``, We, want, to, see, Nelson, Mandela, and, ...\n",
       "\n",
       "[5462 rows x 2 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data = []\n",
    "sentence = []\n",
    "pos_tags = []\n",
    "\n",
    "sentence_count = 1\n",
    "with open('data/test', newline = '') as tsvfile:                            # read test file line by line \n",
    "    csv_reader = csv.reader(tsvfile, delimiter = '\\t')                      # create sentences df like train_data\n",
    "    for row in csv_reader:\n",
    "        try:\n",
    "            sentence.append(row[1])\n",
    "        except Exception as e:\n",
    "            test_data.append([sentence_count, sentence])\n",
    "            sentence_count += 1\n",
    "            sentence = []\n",
    "            pos_tags = []\n",
    "        \n",
    "    test_data.append([sentence_count, sentence]) \n",
    "    \n",
    "test_df = pd.DataFrame(test_data,columns = ['sentence_number', 'words'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "79a17f51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "\n",
    "def greedy_decoding_test(df):\n",
    "      with open('greedy.out', 'w') as tsvfile:\n",
    "        prev_tag = '< * >'                                 # start position tag as prev tag             \n",
    "        predicted_tags = []\n",
    "        sentence_accuracies = [] \n",
    "        total_accuracy = 0\n",
    "        small_no = 0.00000001                             # giving a very small probability to non existent proabilities to preserve matched emission / transition probabilities\n",
    "\n",
    "        for idx, (_, words) in df.iterrows():   # loop through the data \n",
    "            predicted_tags = []                           # keep track of predicted_tags so far\n",
    "            prev_tag = '< * >'                            # start with prev tag as star position tag\n",
    "            for word in words:                            # go word by word in the sentence\n",
    "                max_p = 0                                 # variable to keep track of max probability seen so far\n",
    "                best_tag = random.choice(list(first_tags))       # randomly choose a start POS tag\n",
    "                for pos_tag in pos_freq.keys():            # check probabilties of each plausible tag\n",
    "                    if word not in vocab:\n",
    "                        word = '< unk >'                   # give < unk > frequency value for words not seen \n",
    "                    if (prev_tag, pos_tag) in transition.keys() and (pos_tag, word) in emission.keys(): \n",
    "                        p = (transition[(prev_tag, pos_tag)]+small_no) * (emission[(pos_tag, word)]+small_no)  \n",
    "\n",
    "                    elif (pos_tag, word) in emission.keys() :         # if emission is found and transition is not matched\n",
    "                        p = (emission[(pos_tag, word)]+ small_no)  * small_no\n",
    "\n",
    "                    elif (prev_tag, pos_tag) in transition.keys() : # if exits and emission match isnt found\n",
    "                        p = (transition[(prev_tag, pos_tag)]+ small_no) *  small_no\n",
    "\n",
    "\n",
    "                    else:\n",
    "                        p = small_no * small_no\n",
    "                    if p > max_p:                                      # update best probability found \n",
    "                            max_p = p\n",
    "                            best_tag = pos_tag\n",
    "\n",
    "\n",
    "\n",
    "                predicted_tags.append(best_tag)                        # add the best_found tag to the predicted tags list\n",
    "                prev_tag = best_tag                                    # update prev tag as curr predicted tag\n",
    "\n",
    "            predicted_tags.append(predicted_tags)\n",
    "            idx = 0\n",
    "            for word, pos_tag in zip(words, predicted_tags):\n",
    "                idx += 1\n",
    "                print (\"%d\\t%s\\t%s\" % (idx, word, pos_tag), file=tsvfile)\n",
    "            print (\"\", file=tsvfile)\n",
    "                \n",
    "        \n",
    "greedy_decoding_test(test_df) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9b3590c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi_decoding_test(df):\n",
    "    \n",
    "    prev_tag= '< * >'          # start position tag as prev tag                                      \n",
    "    predicted_tags = []        # save the predicted tags \n",
    "    sentence_accuracies = []   # append the sentence accuracies\n",
    "    total_accuracy = 0\n",
    "    num = 0\n",
    "    small_no = 0.00000001 \n",
    "    \n",
    "    with open('viterbi.out', 'w') as tsvfile:              # open the .out file \n",
    "        for idx, (_, words) in df.iterrows():              # read the df line by line \n",
    "                predicted_tags = []                        \n",
    "                T = len(words) \n",
    "                N = len(pos_freq.items())\n",
    "                viterbi = [[0]*(T) for _ in range(N)]\n",
    "                path_tracker = [[0]*(T) for _ in range(N)]\n",
    "                for i in range(0,len(path_tracker)):\n",
    "                    path_tracker[i][0] = -1\n",
    "\n",
    "                for t, word in enumerate(words):           # for each timestamp , word in words\n",
    "                    if word not in vocab:                  # if word is not in vocab give it  < unk > tag\n",
    "                        word = '< unk >'\n",
    "                    for s1, pos_tag in enumerate(pos_freq.keys()):   # for every state, current tag  \n",
    "                        if pos_tag == '< * >':\n",
    "                            pass\n",
    "                        elif t == 0:                          # handle first word separately\n",
    "                            if ('< * >', pos_tag) in transition.keys() and (pos_tag, word) in emission.keys():\n",
    "                                viterbi[s1][0] = (transition[('< * >', pos_tag)] +small_no) * (emission[(pos_tag, word)]+small_no)\n",
    "                                path_tracker[s1][0] = -1      # to track end when backtracking to find the path\n",
    "                        else:\n",
    "                            for s2, prev_tag in enumerate(pos_freq.keys()): # for every possible prev state, prev tag \n",
    "                                if prev_tag == '< * >':                     # ignore start symbol tag\n",
    "                                    pass\n",
    "                                elif (prev_tag, pos_tag) in transition.keys() and (pos_tag, word) in emission.keys():\n",
    "                                    p = viterbi[s2][t-1]*(transition[(prev_tag, pos_tag)]+small_no) * (emission[(pos_tag, word)]+small_no)\n",
    "                                    if p  > viterbi[s1][t]:              # save the best transition probability in the the viterbi table\n",
    "                                        viterbi[s1][t] = p\n",
    "                                        path_tracker[s1][t] = s2         # save best prev pos in this path\n",
    "\n",
    "                best_path_p = 0\n",
    "                best_path_index = 0\n",
    "                for i in range(N):                                       # find the best path start by looking at the last column in viterbi table\n",
    "                    if viterbi[i][T-1] > best_path_p:                    \n",
    "                        best_path_p = viterbi[i][T-1]\n",
    "                        best_path_index = i\n",
    "\n",
    "                best_path = []\n",
    "                t = T-1\n",
    "                \n",
    "                pos_list = list(pos_freq.keys()) \n",
    "                while best_path_index != -1 and t!=-1:                             # backtrack to find the path that gave the best probability \n",
    "                    best_path.append(pos_list[best_path_index])\n",
    "                    best_path_index = path_tracker[best_path_index][t]\n",
    "                    t -= 1 \n",
    "                if len(best_path) != len(words) :\n",
    "                    print(\"best path not found\")\n",
    "\n",
    "\n",
    "\n",
    "                best_path = best_path[::-1]                              # reverse the best path tags found by backtracking                          \n",
    "                idx = 0\n",
    "                for word, pos_tag in zip(words, best_path):\n",
    "                    idx += 1\n",
    "                    print (\"%d\\t%s\\t%s\" % (idx, word, pos_tag), file=tsvfile)\n",
    "                print (\"\", file=tsvfile)\n",
    "\n",
    "\n",
    "\n",
    "viterbi_decoding_test(test_df)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36ada24",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
